{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+ih3UezFNpz7C0m38VepA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blue99999/AI-class/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_CazTkJ63A",
        "outputId": "0e5e13d5-313f-4060-f391-4aa141fa43f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-23 15:30:52--  https://github.com/ADlead/Dogs-Cats/archive/master.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/ADlead/Dogs-Cats/zip/refs/heads/master [following]\n",
            "--2023-10-23 15:31:17--  https://codeload.github.com/ADlead/Dogs-Cats/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 192.30.255.121\n",
            "Connecting to codeload.github.com (codeload.github.com)|192.30.255.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/tmp/cats-and-dogs.zip’\n",
            "\n",
            "/tmp/cats-and-dogs.     [    <=>             ] 817.02M  24.4MB/s    in 29s     \n",
            "\n",
            "2023-10-23 15:31:47 (28.2 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [856708511]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n",
        "# This will require you doing a lot of data preprocessing because\n",
        "# the dataset isn't split into training and validation for you\n",
        "# This code block has all the required inputs\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "# This code block downloads the full Cats-v-Dogs dataset and stores it as\n",
        "# cats-and-dogs.zip. It then unzips it to /tmp\n",
        "# which will create a tmp/PetImages directory containing subdirectories\n",
        "# called 'Cat' and 'Dog' (that's how the original researchers structured it)\n",
        "# If the URL doesn't work,\n",
        "# .  visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
        "# And right click on the 'Download Manually' link to get a new URL\n",
        "!wget --no-check-certificate \\\n",
        "  \"https://github.com/ADlead/Dogs-Cats/archive/master.zip\" \\\n",
        "  -O \"/tmp/cats-and-dogs.zip\"\n",
        "\n",
        "local_zip = '/tmp/cats-and-dogs.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "data_dir = '/tmp/Dogs-Cats-master/data'\n",
        "batch_save_path = '/tmp/Dogs-Cats-master/batch_files'\n",
        "\n",
        "# 建立batch檔案儲存的資料夾\n",
        "os.makedirs(batch_save_path, exist_ok=True)\n",
        "\n",
        "# 圖片統一大小：100 * 100\n",
        "# 訓練集20000：100個batch文件，每個文件200張圖片\n",
        "# 驗證集5000： 一個測試文件，測試時50張x 100 批次\n",
        "\n",
        "# 進入圖片資料的目錄，讀取圖片訊息\n",
        "all_data_files = os.listdir(os.path.join(data_dir, 'train/'))\n",
        "\n",
        "# print(all_data_files)\n",
        "\n",
        "# 打算資料的順序\n",
        "random.shuffle(all_data_files)\n",
        "\n",
        "all_train_files = all_data_files[:20000]\n",
        "all_test_files = all_data_files[20000:]\n",
        "\n",
        "train_data = []\n",
        "train_label = []\n",
        "train_filenames = []\n",
        "\n",
        "test_data = []\n",
        "test_label = []\n",
        "test_filenames = []\n",
        "\n",
        "# 訓練集\n",
        "for each in all_train_files:\n",
        "  img = cv.imread(os.path.join(data_dir,'train/',each),1)\n",
        "  resized_img = cv.resize(img, (100,100))\n",
        "\n",
        "  img_data = np.array(resized_img)\n",
        "  train_data.append(img_data)\n",
        "  if 'cat' in each:\n",
        "    train_label.append(0)\n",
        "  elif 'dog' in each:\n",
        "    train_label.append(1)\n",
        "  else:\n",
        "    raise Exception('%s is wrong train file'%(each))\n",
        "  train_filenames.append(each)\n",
        "\n",
        "# 測試集\n",
        "for each in all_test_files:\n",
        "  img = cv.imread(os.path.join(data_dir,'train/',each), 1)\n",
        "  resized_img = cv.resize(img, (100,100))\n",
        "\n",
        "  img_data = np.array(resized_img)\n",
        "  test_data.append(img_data)\n",
        "  if 'cat' in each:\n",
        "    test_label.append(0)\n",
        "  elif 'dog' in each:\n",
        "    test_label.append(1)\n",
        "  else:\n",
        "    raise Exception('%s is wrong test file'%(each))\n",
        "  test_filenames.append(each)\n",
        "\n",
        "print(len(train_data), len(test_data))\n",
        "\n",
        "# 製作100個batch文件\n",
        "start = 0\n",
        "end = 200\n",
        "for num in range(1, 101):\n",
        "  batch_data = train_data[start: end]\n",
        "  batch_label = train_label[start: end]\n",
        "  batch_filenames = train_filenames[start: end]\n",
        "  batch_name = 'training batch {} of 15'.format(num)\n",
        "\n",
        "  all_data = {\n",
        "    'data':batch_data,\n",
        "    'label':batch_label,\n",
        "    'filenames':batch_filenames,\n",
        "    'name':batch_name\n",
        "  }\n",
        "\n",
        "  with open(os.path.join(batch_save_path, 'train_batch_{}'.format(num)), 'wb') as f:\n",
        "    pickle.dump(all_data, f)\n",
        "\n",
        "  start += 200\n",
        "  end += 200\n",
        "\n",
        "# 製作測試文件\n",
        "all_test_data = {\n",
        "  'data':test_data,\n",
        "  'label':test_label,\n",
        "  'filenames':test_filenames,\n",
        "  '名':'test batch 1 of 1'\n",
        "}\n",
        "\n",
        "with open(os.path.join(batch_save_path, 'test_batch'), 'wb') as f:\n",
        "  pickle.dump(all_test_data, f)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print('製作結束, 用時{}秒'.format(end_time - start_time))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfTprl8IKBgk",
        "outputId": "17752728-5713-4587-b63a-b99b99f34df0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 5000\n",
            "製作結束, 用時52.1927125453949秒\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "''' 全域參數'''\n",
        "IMAGE_SIZE = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "TRAIN_STEP = 10000\n",
        "TRAIN_SIZE = 100\n",
        "TEST_STEP = 100\n",
        "TEST_SIZE = 50\n",
        "IS_TRAIN = True\n",
        "\n",
        "SAVE_PATH = '/tmp/Dogs-Cats-master/model/'\n",
        "\n",
        "data_dir = '/tmp/Dogs-Cats-master/batch_files'\n",
        "pic_path = '/tmp/Dogs-Cats-master/data/test1'\n",
        "\n",
        "''''''\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "  '''從batch檔案讀取圖片資訊'''\n",
        "  with open(filename, 'rb') as f:\n",
        "    data = pickle.load(f, encoding='iso-8859-1')\n",
        "    return data['data'],data['label'],data['filenames']\n",
        "\n",
        "# 讀取資料的類\n",
        "class InputData:\n",
        "  def __init__(self, filenames, need_shuffle):\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "    all_names = []\n",
        "    for file in filenames:\n",
        "      data, labels, filename = load_data(file)\n",
        "\n",
        "      all_data.append(data)\n",
        "      all_labels.append(labels)\n",
        "      all_names += filename\n",
        "\n",
        "    self._data = np.vstack(all_data)\n",
        "    self._labels = np.hstack(all_labels)\n",
        "    print(self._data.shape)\n",
        "    print(self._labels.shape)\n",
        "\n",
        "    self._filenames = all_names\n",
        "\n",
        "    self._num_examples = self._data.shape[0]\n",
        "    self._need_shuffle = need_shuffle\n",
        "    self._indicator = 0\n",
        "    if self._indicator:\n",
        "      self._shuffle_data()\n",
        "\n",
        "  def _shuffle_data(self):\n",
        "    # 把數據再混排\n",
        "    p = np.random.permutation(self._num_examples)\n",
        "    self._data = self._data[p]\n",
        "    self._labels = self._labels[p]\n",
        "\n",
        "  def next_batch(self, batch_size):\n",
        "    '''傳回每一批的資料'''\n",
        "    end_indicator = self._indicator + batch_size\n",
        "    if end_indicator > self._num_examples:\n",
        "      if self._need_shuffle:\n",
        "        self._shuffle_data()\n",
        "        self._indicator = 0\n",
        "        end_indicator = batch_size\n",
        "      else:\n",
        "        raise Exception('have no more examples')\n",
        "    if end_indicator > self._num_examples:\n",
        "      raise Exception('batch size is larger than all examples')\n",
        "    batch_data = self._data[self._indicator : end_indicator]\n",
        "    batch_labels = self._labels[self._indicator : end_indicator]\n",
        "    batch_filenames = self._filenames[self._indicator : end_indicator]\n",
        "    self._indicator = end_indicator\n",
        "    return batch_data, batch_labels, batch_filenames\n",
        "\n",
        "# 定義一個類別\n",
        "class MyTensor:\n",
        "  def __init__(self):\n",
        "\n",
        "\n",
        "    # 載入訓練集和測試集\n",
        "    train_filenames = [os.path.join(data_dir, 'train_batch_%d'%i) for i in range(1, 101)]\n",
        "    test_filenames = [os.path.join(data_dir, 'test_batch')]\n",
        "    self.batch_train_data = InputData(train_filenames, True)\n",
        "    self.batch_test_data = InputData(test_filenames, True)\n",
        "\n",
        "    pass\n",
        "\n",
        "  def flow(self):\n",
        "    self.x = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, 3], 'input_data')\n",
        "    self.y = tf.placeholder(tf.int64, [None], 'output_data')\n",
        "    self.keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "    # self.x = self.x / 255.0 需不需要這一步？\n",
        "\n",
        "    # 圖片輸入網路中\n",
        "    fc = self.conv_net(self.x, self.keep_prob)\n",
        "\n",
        "    self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.y, logits=fc)\n",
        "    self.y_ = tf.nn.softmax(fc) # 計算每一類的機率\n",
        "    self.predict = tf.argmax(fc, 1)\n",
        "    self.acc = tf.reduce_mean(tf.cast(tf.equal(self.predict, self.y), tf.float32))\n",
        "\n",
        "    self.train_op = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)\n",
        "    self.saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "    print('計算流程圖已經建造.')\n",
        "\n",
        "  # 訓練\n",
        "  def myTrain(self):\n",
        "    acc_list = []\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "\n",
        "      for i in range(TRAIN_STEP):\n",
        "        train_data, train_label, _ = self.batch_train_data.next_batch(TRAIN_SIZE)\n",
        "\n",
        "        eval_ops = [self.loss, self.acc, self.train_op]\n",
        "        eval_ops_results = sess.run(eval_ops, feed_dict={\n",
        "          self.x:train_data,\n",
        "          self.y:train_label,\n",
        "          self.keep_prob:0.7\n",
        "        })\n",
        "        loss_val, train_acc = eval_ops_results[0:2]\n",
        "\n",
        "        acc_list.append(train_acc)\n",
        "        if (i+1) % 100 == 0:\n",
        "          acc_mean = np.mean(acc_list)\n",
        "          print('step:{0},loss:{1:.5},acc:{2:.5},acc_mean:{3:.5}'.format(\n",
        "            i+1,loss_val,train_acc,acc_mean\n",
        "          ))\n",
        "        if (i+1) % 1000 == 0:\n",
        "          test_acc_list = []\n",
        "          for j in range(TEST_STEP):\n",
        "            test_data, test_label, _ = self.batch_test_data.next_batch(TRAIN_SIZE)\n",
        "            acc_val = sess.run([self.acc],feed_dict={\n",
        "              self.x:test_data,\n",
        "              self.y:test_label,\n",
        "              self.keep_prob:1.0\n",
        "            })\n",
        "            test_acc_list.append(acc_val)\n",
        "          print('[Test ] step:{0}, mean_acc:{1:.5}'.format(\n",
        "            i+1, np.mean(test_acc_list)\n",
        "          ))\n",
        "      # 儲存訓練後的模型\n",
        "      os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "      self.saver.save(sess, SAVE_PATH + 'my_model.ckpt')\n",
        "\n",
        "  def myTest(self):\n",
        "    with tf.Session() as sess:\n",
        "      model_file = tf.train.latest_checkpoint(SAVE_PATH)\n",
        "      model = self.saver.restore(sess, save_path=model_file)\n",
        "      test_acc_list = []\n",
        "      predict_list = []\n",
        "      for j in range(TEST_STEP):\n",
        "        test_data, test_label, test_name = self.batch_test_data.next_batch(TEST_SIZE)\n",
        "        for each_data, each_label, each_name in zip(test_data, test_label, test_name):\n",
        "          acc_val, y__, pre, test_img_data = sess.run(\n",
        "            [self.acc, self.y_, self.predict, self.x],\n",
        "            feed_dict={\n",
        "              self.x:each_data.reshape(1, IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "              self.y:each_label.reshape(1),\n",
        "              self.keep_prob:1.0\n",
        "            }\n",
        "          )\n",
        "          predict_list.append(pre[0])\n",
        "          test_acc_list.append(acc_val)\n",
        "\n",
        "          # 把測試結果顯示出來\n",
        "          self.compare_test(test_img_data, each_label, pre[0], y__[0], each_name)\n",
        "      print('[Test ] mean_acc:{0:.5}'.format(np.mean(test_acc_list)))\n",
        "\n",
        "  def compare_test(self, input_image_arr, input_label, output, probability, img_name):\n",
        "    classes = ['cat', 'dog']\n",
        "    if input_label == output:\n",
        "      result = '正確'\n",
        "    else:\n",
        "      result = '錯誤'\n",
        "    print('測試【{0}】,輸入的label:{1}, 預測得是{2}:{3}的機率:{4:.5}, 輸入的圖片名稱:{5}'.format(\n",
        "      result,input_label, output,classes[output], probability[output], img_name\n",
        "    ))\n",
        "\n",
        "  def conv_net(self, x, keep_prob):\n",
        "    conv1_1 = tf.layers.conv2d(x, 16, (3, 3), padding='same', activation=tf.nn.relu, name='conv1_1')\n",
        "    conv1_2 = tf.layers.conv2d(conv1_1, 16, (3, 3), padding='same', activation=tf.nn.relu, name='conv1_2')\n",
        "    pool1 = tf.layers.max_pooling2d(conv1_2, (2, 2), (2, 2), name='pool1')\n",
        "\n",
        "    conv2_1 = tf.layers.conv2d(pool1, 32, (3, 3), padding='same', activation=tf.nn.relu, name='conv2_1')\n",
        "    conv2_2 = tf.layers.conv2d(conv2_1, 32, (3, 3), padding='same', activation=tf.nn.relu, name='conv2_2')\n",
        "    pool2 = tf.layers.max_pooling2d(conv2_2, (2, 2), (2, 2), name='pool2')\n",
        "\n",
        "    conv3_1 = tf.layers.conv2d(pool2, 64, (3, 3), padding='same', activation=tf.nn.relu, name='conv3_1')\n",
        "    conv3_2 = tf.layers.conv2d(conv3_1, 64, (3, 3), padding='same', activation=tf.nn.relu, name='conv3_2')\n",
        "    pool3 = tf.layers.max_pooling2d(conv3_2, (2, 2), (2, 2), name='pool3')\n",
        "\n",
        "    conv4_1 = tf.layers.conv2d(pool3, 128, (3, 3), padding='same', activation=tf.nn.relu, name='conv4_1')\n",
        "    conv4_2 = tf.layers.conv2d(conv4_1, 128, (3, 3), padding='same', activation=tf.nn.relu, name='conv4_2')\n",
        "    pool4 = tf.layers.max_pooling2d(conv4_2, (2, 2), (2, 2), name='pool4')\n",
        "\n",
        "    flatten = tf.layers.flatten(pool4) # 把網路展平，以輸入到後面的全連接層\n",
        "\n",
        "    fc1 = tf.layers.dense(flatten, 512, tf.nn.relu)\n",
        "    fc1_dropout = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
        "    fc2 = tf.layers.dense(fc1, 256, tf.nn.relu)\n",
        "    fc2_dropout = tf.nn.dropout(fc2, keep_prob=keep_prob)\n",
        "    fc3 = tf.layers.dense(fc2, 2, None) # 得到輸出fc3\n",
        "\n",
        "    return fc3\n",
        "\n",
        "  def main(self):\n",
        "    self.flow()\n",
        "    if IS_TRAIN is True:\n",
        "      self.myTrain()\n",
        "    else:\n",
        "      self.myTest()\n",
        "\n",
        "  def final_classify(self):\n",
        "    all_test_files_dir = './data/test1'\n",
        "    all_test_filenames = os.listdir(all_test_files_dir)\n",
        "    if IS_TRAIN is False:\n",
        "      self.flow()\n",
        "      # self.classify()\n",
        "      with tf.Session() as sess:\n",
        "        model_file = tf.train.latest_checkpoint(SAVE_PATH)\n",
        "        mpdel = self.saver.restore(sess,save_path=model_file)\n",
        "\n",
        "        predict_list = []\n",
        "        for each_filename in all_test_filenames:\n",
        "          each_data = self.get_img_data(os.path.join(all_test_files_dir,each_filename))\n",
        "          y__, pre, test_img_data = sess.run(\n",
        "            [self.y_, self.predict, self.x],\n",
        "            feed_dict={\n",
        "              self.x:each_data.reshape(1, IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "              self.keep_prob: 1.0\n",
        "            }\n",
        "          )\n",
        "          predict_list.append(pre[0])\n",
        "          self.classify(test_img_data, pre[0], y__[0], each_filename)\n",
        "\n",
        "    else:\n",
        "      print('now is training model...')\n",
        "\n",
        "  def classify(self, input_image_arr, output, probability, img_name):\n",
        "    classes = ['cat','dog']\n",
        "    single_image = input_image_arr[0] #* 255\n",
        "    if output == 0:\n",
        "      output_dir = 'cat/'\n",
        "    else:\n",
        "      output_dir = 'dog/'\n",
        "    os.makedirs(os.path.join('./classiedResult', output_dir), exist_ok=True)\n",
        "    cv.imwrite(os.path.join('./classiedResult',output_dir, img_name),single_image)\n",
        "    print('輸入的圖片名稱:{0}，預測得有{1:5}的機率是{2}:{3}'.format(\n",
        "      img_name,\n",
        "      probability[output],\n",
        "      output,\n",
        "      classes[output]\n",
        "    ))\n",
        "\n",
        "  # 根據名稱取得圖片像素\n",
        "  def get_img_data(self,img_name):\n",
        "    img = cv.imread(img_name)\n",
        "    resized_img = cv.resize(img, (100, 100))\n",
        "    img_data = np.array(resized_img)\n",
        "\n",
        "    return img_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  mytensor = MyTensor()\n",
        "  mytensor.main() # 用於訓練或測試\n",
        "\n",
        "  # mytensor.final_classify() # 用於最後的分類\n",
        "\n",
        "  print('hello world')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kREYwnWtMpUk",
        "outputId": "ec7038dc-fe1f-47b4-9606-77fa87f31598"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100, 100, 3)\n",
            "(20000,)\n",
            "(5000, 100, 100, 3)\n",
            "(5000,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-48a986a765d5>:189: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv1_1 = tf.layers.conv2d(x, 16, (3, 3), padding='same', activation=tf.nn.relu, name='conv1_1')\n",
            "<ipython-input-14-48a986a765d5>:190: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv1_2 = tf.layers.conv2d(conv1_1, 16, (3, 3), padding='same', activation=tf.nn.relu, name='conv1_2')\n",
            "<ipython-input-14-48a986a765d5>:191: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  pool1 = tf.layers.max_pooling2d(conv1_2, (2, 2), (2, 2), name='pool1')\n",
            "<ipython-input-14-48a986a765d5>:193: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv2_1 = tf.layers.conv2d(pool1, 32, (3, 3), padding='same', activation=tf.nn.relu, name='conv2_1')\n",
            "<ipython-input-14-48a986a765d5>:194: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv2_2 = tf.layers.conv2d(conv2_1, 32, (3, 3), padding='same', activation=tf.nn.relu, name='conv2_2')\n",
            "<ipython-input-14-48a986a765d5>:195: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  pool2 = tf.layers.max_pooling2d(conv2_2, (2, 2), (2, 2), name='pool2')\n",
            "<ipython-input-14-48a986a765d5>:197: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv3_1 = tf.layers.conv2d(pool2, 64, (3, 3), padding='same', activation=tf.nn.relu, name='conv3_1')\n",
            "<ipython-input-14-48a986a765d5>:198: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv3_2 = tf.layers.conv2d(conv3_1, 64, (3, 3), padding='same', activation=tf.nn.relu, name='conv3_2')\n",
            "<ipython-input-14-48a986a765d5>:199: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  pool3 = tf.layers.max_pooling2d(conv3_2, (2, 2), (2, 2), name='pool3')\n",
            "<ipython-input-14-48a986a765d5>:201: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv4_1 = tf.layers.conv2d(pool3, 128, (3, 3), padding='same', activation=tf.nn.relu, name='conv4_1')\n",
            "<ipython-input-14-48a986a765d5>:202: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv4_2 = tf.layers.conv2d(conv4_1, 128, (3, 3), padding='same', activation=tf.nn.relu, name='conv4_2')\n",
            "<ipython-input-14-48a986a765d5>:203: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  pool4 = tf.layers.max_pooling2d(conv4_2, (2, 2), (2, 2), name='pool4')\n",
            "<ipython-input-14-48a986a765d5>:205: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  flatten = tf.layers.flatten(pool4) # 把網路展平，以輸入到後面的全連接層\n",
            "<ipython-input-14-48a986a765d5>:207: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  fc1 = tf.layers.dense(flatten, 512, tf.nn.relu)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "<ipython-input-14-48a986a765d5>:209: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  fc2 = tf.layers.dense(fc1, 256, tf.nn.relu)\n",
            "<ipython-input-14-48a986a765d5>:211: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  fc3 = tf.layers.dense(fc2, 2, None) # 得到輸出fc3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "計算流程圖已經建造.\n",
            "step:100,loss:0.60888,acc:0.69,acc_mean:0.5969\n",
            "step:200,loss:0.60043,acc:0.68,acc_mean:0.6333\n",
            "step:300,loss:0.56113,acc:0.65,acc_mean:0.65463\n",
            "step:400,loss:0.49622,acc:0.74,acc_mean:0.6714\n",
            "step:500,loss:0.454,acc:0.77,acc_mean:0.6894\n",
            "step:600,loss:0.46926,acc:0.78,acc_mean:0.70213\n",
            "step:700,loss:0.37615,acc:0.85,acc_mean:0.71553\n",
            "step:800,loss:0.47474,acc:0.75,acc_mean:0.72565\n",
            "step:900,loss:0.42712,acc:0.81,acc_mean:0.73703\n",
            "step:1000,loss:0.33696,acc:0.86,acc_mean:0.74589\n",
            "[Test ] step:1000, mean_acc:0.7926\n",
            "step:1100,loss:0.42175,acc:0.76,acc_mean:0.75604\n",
            "step:1200,loss:0.28815,acc:0.88,acc_mean:0.76421\n",
            "step:1300,loss:0.24076,acc:0.89,acc_mean:0.77318\n",
            "step:1400,loss:0.29489,acc:0.88,acc_mean:0.78083\n",
            "step:1500,loss:0.25173,acc:0.88,acc_mean:0.7893\n",
            "step:1600,loss:0.17942,acc:0.96,acc_mean:0.79663\n",
            "step:1700,loss:0.43088,acc:0.78,acc_mean:0.80442\n",
            "step:1800,loss:0.16519,acc:0.93,acc_mean:0.8112\n",
            "step:1900,loss:0.11299,acc:0.96,acc_mean:0.81876\n",
            "step:2000,loss:0.17837,acc:0.91,acc_mean:0.8252\n",
            "[Test ] step:2000, mean_acc:0.7958\n",
            "step:2100,loss:0.015827,acc:1.0,acc_mean:0.83224\n",
            "step:2200,loss:0.11821,acc:0.95,acc_mean:0.83777\n",
            "step:2300,loss:0.043195,acc:0.98,acc_mean:0.844\n",
            "step:2400,loss:0.084822,acc:0.96,acc_mean:0.84944\n",
            "step:2500,loss:0.099749,acc:0.97,acc_mean:0.85486\n",
            "step:2600,loss:0.071141,acc:0.97,acc_mean:0.8596\n",
            "step:2700,loss:0.043814,acc:0.98,acc_mean:0.86457\n",
            "step:2800,loss:0.056705,acc:0.97,acc_mean:0.86898\n",
            "step:2900,loss:0.032706,acc:0.99,acc_mean:0.87307\n",
            "step:3000,loss:0.056176,acc:0.98,acc_mean:0.87692\n",
            "[Test ] step:3000, mean_acc:0.793\n",
            "step:3100,loss:0.054603,acc:0.98,acc_mean:0.88047\n",
            "step:3200,loss:0.025071,acc:0.99,acc_mean:0.88363\n",
            "step:3300,loss:0.056893,acc:0.97,acc_mean:0.88692\n",
            "step:3400,loss:0.010941,acc:1.0,acc_mean:0.89005\n",
            "step:3500,loss:0.12012,acc:0.95,acc_mean:0.89305\n",
            "step:3600,loss:0.031063,acc:1.0,acc_mean:0.89565\n",
            "step:3700,loss:0.021399,acc:0.99,acc_mean:0.89817\n",
            "step:3800,loss:0.02637,acc:0.98,acc_mean:0.90058\n",
            "step:3900,loss:0.038797,acc:0.97,acc_mean:0.9029\n",
            "step:4000,loss:0.054988,acc:0.98,acc_mean:0.90494\n",
            "[Test ] step:4000, mean_acc:0.7892\n",
            "step:4100,loss:0.014465,acc:1.0,acc_mean:0.90713\n",
            "step:4200,loss:0.04627,acc:0.98,acc_mean:0.90912\n",
            "step:4300,loss:0.03088,acc:0.99,acc_mean:0.91086\n",
            "step:4400,loss:0.019834,acc:0.99,acc_mean:0.9125\n",
            "step:4500,loss:0.0071032,acc:1.0,acc_mean:0.91438\n",
            "step:4600,loss:0.00085722,acc:1.0,acc_mean:0.91618\n",
            "step:4700,loss:0.00045622,acc:1.0,acc_mean:0.91794\n",
            "step:4800,loss:0.03408,acc:0.98,acc_mean:0.9196\n",
            "step:4900,loss:0.0088903,acc:1.0,acc_mean:0.92112\n",
            "step:5000,loss:0.012955,acc:1.0,acc_mean:0.92224\n",
            "[Test ] step:5000, mean_acc:0.7946\n",
            "step:5100,loss:0.012974,acc:1.0,acc_mean:0.92365\n",
            "step:5200,loss:0.055017,acc:0.98,acc_mean:0.92497\n",
            "step:5300,loss:0.040013,acc:0.98,acc_mean:0.9262\n",
            "step:5400,loss:0.043398,acc:0.97,acc_mean:0.92745\n",
            "step:5500,loss:0.094174,acc:0.94,acc_mean:0.92861\n",
            "step:5600,loss:0.0096985,acc:1.0,acc_mean:0.92969\n",
            "step:5700,loss:0.023839,acc:0.99,acc_mean:0.93069\n",
            "step:5800,loss:0.0022423,acc:1.0,acc_mean:0.9317\n",
            "step:5900,loss:0.0060612,acc:1.0,acc_mean:0.93284\n",
            "step:6000,loss:0.0012554,acc:1.0,acc_mean:0.93393\n",
            "[Test ] step:6000, mean_acc:0.7968\n",
            "step:6100,loss:0.00054898,acc:1.0,acc_mean:0.93501\n",
            "step:6200,loss:0.00073334,acc:1.0,acc_mean:0.93606\n",
            "step:6300,loss:0.00010585,acc:1.0,acc_mean:0.93708\n",
            "step:6400,loss:9.2329e-05,acc:1.0,acc_mean:0.93806\n",
            "step:6500,loss:0.00010637,acc:1.0,acc_mean:0.93901\n",
            "step:6600,loss:4.8773e-05,acc:1.0,acc_mean:0.93994\n",
            "step:6700,loss:2.5024e-05,acc:1.0,acc_mean:0.94083\n",
            "step:6800,loss:2.6914e-05,acc:1.0,acc_mean:0.9417\n",
            "step:6900,loss:3.3195e-05,acc:1.0,acc_mean:0.94255\n",
            "step:7000,loss:9.824e-05,acc:1.0,acc_mean:0.94337\n",
            "[Test ] step:7000, mean_acc:0.8126\n",
            "step:7100,loss:7.3539e-05,acc:1.0,acc_mean:0.94417\n",
            "step:7200,loss:9.5297e-05,acc:1.0,acc_mean:0.94494\n",
            "step:7300,loss:1.4708e-05,acc:1.0,acc_mean:0.9457\n",
            "step:7400,loss:2.3936e-05,acc:1.0,acc_mean:0.94643\n",
            "step:7500,loss:1.9013e-05,acc:1.0,acc_mean:0.94714\n",
            "step:7600,loss:4.8512e-05,acc:1.0,acc_mean:0.94784\n",
            "step:7700,loss:1.9688e-05,acc:1.0,acc_mean:0.94852\n",
            "step:7800,loss:2.8395e-05,acc:1.0,acc_mean:0.94918\n",
            "step:7900,loss:3.1328e-05,acc:1.0,acc_mean:0.94982\n",
            "step:8000,loss:1.3518e-05,acc:1.0,acc_mean:0.95045\n",
            "[Test ] step:8000, mean_acc:0.8122\n",
            "step:8100,loss:1.1693e-05,acc:1.0,acc_mean:0.95106\n",
            "step:8200,loss:3.1398e-05,acc:1.0,acc_mean:0.95166\n",
            "step:8300,loss:8.5313e-06,acc:1.0,acc_mean:0.95224\n",
            "step:8400,loss:1.0642e-05,acc:1.0,acc_mean:0.95281\n",
            "step:8500,loss:2.0391e-05,acc:1.0,acc_mean:0.95336\n",
            "step:8600,loss:1.9164e-05,acc:1.0,acc_mean:0.9539\n",
            "step:8700,loss:5.5346e-06,acc:1.0,acc_mean:0.95443\n",
            "step:8800,loss:2.2375e-05,acc:1.0,acc_mean:0.95495\n",
            "step:8900,loss:6.7243e-06,acc:1.0,acc_mean:0.95546\n",
            "step:9000,loss:1.7598e-05,acc:1.0,acc_mean:0.95595\n",
            "[Test ] step:9000, mean_acc:0.8142\n",
            "step:9100,loss:1.2469e-05,acc:1.0,acc_mean:0.95644\n",
            "step:9200,loss:1.2302e-05,acc:1.0,acc_mean:0.95691\n",
            "step:9300,loss:1.0715e-05,acc:1.0,acc_mean:0.95737\n",
            "step:9400,loss:1.8106e-05,acc:1.0,acc_mean:0.95783\n",
            "step:9500,loss:1.8366e-05,acc:1.0,acc_mean:0.95827\n",
            "step:9600,loss:1.2984e-05,acc:1.0,acc_mean:0.95871\n",
            "step:9700,loss:6.2999e-06,acc:1.0,acc_mean:0.95913\n",
            "step:9800,loss:3.5571e-06,acc:1.0,acc_mean:0.95955\n",
            "step:9900,loss:1.1802e-05,acc:1.0,acc_mean:0.95996\n",
            "step:10000,loss:1.0487e-05,acc:1.0,acc_mean:0.96036\n",
            "[Test ] step:10000, mean_acc:0.8124\n",
            "hello world\n"
          ]
        }
      ]
    }
  ]
}